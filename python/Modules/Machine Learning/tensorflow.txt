figures out backpropagation 
tensorflow datasets are generators -> can't directly access contents without iterating
    must create next(iter())


import numpy as np 
import tensorflow as tf






tf
    .__version__
    .Tensor
        data and information about the computational graph
    .Variable(value, dtype)

        w = tf.Variable(0, dtype = tf.float32)
        cost = w ** 2 - 10 * w + 25
    .constant()
        X = tf.constant(np.random.randn(3, 1), name = "X")
    .data
        .Dataset
            .from_tensor_slices()
                x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])
                x_train.element_spec
                x_train.map(normalize_func)
                tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)
            .zip()
                dataset = tf.data.Dataset.zip((X_test, Y_test))
                m = dataset.cardinality.numpy()
                minibatches = dataset.batch(minibatch_size).prefetch(8)
            .cardinality()
    
    .keras
        .optimizers
            .Adam(0.1)
                optimizer = tf.keras.optimizers.Adam(0.1)

                .apply_gradients(zip(cost, trainable_variables))
                .minimize(cost_fn, [w])

            Example:
                def training(x, w, optimizer):
                    def cost_fn():
                        cost = x[0] * w ** 2 + x[1] * w + x[2]
                    for i in range(1000):
                        optimizer.minimise(cost_fn, [w])         
                    return w
        
        .activations
            .sigmoid()
            .relu()
        
        .losses
            .categorical_crossentropy( y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)
        
        .initializers
            .GlorotNormal(seed)
                truncated normal distribution centered on 0. stddev = sqrt(2 / (fan_in + fan_out))
                initializer = tf.keras.initializer.GlorotNormal(seed = 1)
                tf.Variable(initializer, shape = ())
            .random_uniform
            .glorot_uniform
            .constant
            .identity

        .metrics
            .CategoricalAccuracy()
                train_accuracy = tf.keras.metrics.CategoricalAccuracy()
                train_accuracy.reset_states()
                train_accuracy.update_state(minibatch_Y, tf.transpose(Z3)) 

        .Input(shape)

        .Sequential()
            model = tf.keras.Squential([])
            model.compile(optimizer, loss, metrics)
            model.summary()
            history = model.fit(X_train, Y_train, epochs, batch_size) # returns history object
            history.history
            model.evaluate(X_test, Y_test)
        
        .layers 
            .Input()
            .Add()
            .Dense()
            .Activation()
            .ZeroPadding2D()
            .Conv2D()
            .BatchNormalization()
            .ReLU()
            .MaxPool2D()
            .MaxPooling2D()
            .GlobalMaxPooling2D()
            .AveragePooling2D()
            .Flatten()

        .models
            .load_model()    
            .Model(inputs, output)
        
        .preprocessing
            import image
        .applications
            .resnet_v2
                .ResNet50V2
                .preprocess_input
                .decode_predictions
    .sigmoid()
    .softmax()
    .GradientTape()
        record sequence of operations as you compute during forward prop and then revisit ops in reverse to compute backprop

            .gradient(cost , trainable_variables)    

            Example  
                def train_step():
                    with tf.GradientTape() as tape:
                        cost = w ** 2 - 10 * w + 25
                    trainable_variables = [w]
                    grads = tape.gradient(cost, trainable_variables)
                    optimizer.apply_gradients(zip(grads, trainable_variables))    
    .math
        .add()
    .linalg
        .matmul()

    .one_hot(labels, depth, axis = 0)
    .cast("...", tf.float32)
    .rehape(tensor, shape)         
    .matmul()
    .add()
    .reduce_rum()
    .transpose()
    .zeros()
    .ones()
           